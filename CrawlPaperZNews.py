import os
import random
import time
import csv
import re
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException
from bs4 import BeautifulSoup
import pytz

vn_timezone = pytz.timezone('Asia/Ho_Chi_Minh')
current_time = datetime.now(vn_timezone)

# X√°c ƒë·ªãnh khung gi·ªù c·ªë ƒë·ªãnh d·ª±a tr√™n gi·ªù hi·ªán t·∫°i
# Chia ng√†y th√†nh c√°c khung 3 ti·∫øng: 0-2, 3-5, 6-8, 9-11, 12-14, 15-17, 18-20, 21-23
current_hour = current_time.hour
time_slot_start_hour = (current_hour // 3) * 3  # L√†m tr√≤n xu·ªëng b·ªôi s·ªë c·ªßa 3

# T·∫°o khung gi·ªù: t·ª´ X:00:00 ƒë·∫øn X+2:59:59
time_start = current_time.replace(hour=time_slot_start_hour, minute=0, second=0, microsecond=0)
time_end = time_start.replace(hour=time_slot_start_hour + 2, minute=59, second=59, microsecond=999999)

print(f"Khung gi·ªù crawl: {time_start.strftime('%Y-%m-%d %H:%M:%S')} ƒë·∫øn {time_end.strftime('%Y-%m-%d %H:%M:%S')}")

csv_file = 'dataset_paper_znews.csv'
base_url = 'https://znews.vn'

# Danh s√°ch danh m·ª•c c·∫ßn lo·∫°i b·ªè
EXCLUDED_CATEGORIES = [
    'Xu·∫•t b·∫£n',
    'T√°c gi·∫£',
    'Th·∫ø gi·ªõi s√°ch',
    'Cu·ªën s√°ch t√¥i ƒë·ªçc',
    'Nghi√™n c·ª©u xu·∫•t b·∫£n',
]

# üõ† H√†m kh·ªüi t·∫°o driver
def init_driver():
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--disable-extensions")
    options.add_argument("--ignore-certificate-errors")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--dns-prefetch-disable")
    driver = webdriver.Chrome(options=options)
    driver.set_page_load_timeout(180)
    return driver

# üõ† H√†m ch·ªù ph·∫ßn t·ª≠
def wait_for_element(driver, by, value, timeout=10):
    try:
        return WebDriverWait(driver, timeout).until(EC.presence_of_element_located((by, value)))
    except TimeoutException:
        return None

# üõ† H√†m ƒë·ªçc c√°c URL ƒë√£ crawl t·ª´ file CSV
def load_crawled_urls(csv_file):
    crawled_urls = set()
    try:
        with open(csv_file, mode='r', encoding='utf-8-sig') as file:
            reader = csv.reader(file)
            next(reader, None)
            for row in reader:
                if len(row) >= 2:
                    crawled_urls.add(row[1])
    except FileNotFoundError:
        pass
    return crawled_urls

# üõ† H√†m parse th·ªùi gian t·ª´ text ZNews
def parse_znews_time(time_text):
    """
    Parse th·ªùi gian t·ª´ ZNews format:
    - "02:30 23/11/2024"
    - "23/11/2024, 02:30"
    """
    try:
        time_text = time_text.strip()
        
        # Format: "02:30 23/11/2024"
        match = re.search(r'(\d{1,2}):(\d{2})\s+(\d{1,2})/(\d{1,2})/(\d{4})', time_text)
        if match:
            hour, minute, day, month, year = match.groups()
            return datetime(int(year), int(month), int(day), int(hour), int(minute), tzinfo=vn_timezone)
        
        # Format: "23/11/2024, 02:30"
        match = re.search(r'(\d{1,2})/(\d{1,2})/(\d{4}),?\s+(\d{1,2}):(\d{2})', time_text)
        if match:
            day, month, year, hour, minute = match.groups()
            return datetime(int(year), int(month), int(day), int(hour), int(minute), tzinfo=vn_timezone)
        
        print(f"Kh√¥ng parse ƒë∆∞·ª£c th·ªùi gian: {time_text}")
        return None
        
    except Exception as e:
        print(f"L·ªói khi parse th·ªùi gian '{time_text}': {e}")
        return None

# üõ† H√†m ki·ªÉm tra b√†i vi·∫øt c√≥ trong khung gi·ªù kh√¥ng
def is_in_time_range(article_time):
    """Ki·ªÉm tra xem th·ªùi gian b√†i vi·∫øt c√≥ n·∫±m trong khung gi·ªù [time_start, time_end] kh√¥ng"""
    if not article_time:
        return False
    return time_start <= article_time <= time_end

# üõ† H√†m crawl b√†i b√°o
def crawl_article(driver, article_href, writer, crawled_urls):
    if article_href in crawled_urls:
        print(f"B√†i {article_href} ƒë√£ ƒë∆∞·ª£c crawl, b·ªè qua.")
        return True

    for attempt in range(3):
        try:
            driver.get(article_href)
            wait_for_element(driver, By.CLASS_NAME, "detail-content", timeout=10)

            soup = BeautifulSoup(driver.page_source, 'html.parser')
            
            category_name_elem = soup.select_one('header.the-article-header > p.the-article-category > a')
            category_name = category_name_elem.get_text(strip=True) if category_name_elem else "N/A"
            
            time_elem = soup.select_one('header.the-article-header > ul.the-article-meta > li.the-article-publish')
            time_paper = time_elem.get_text(strip=True) if time_elem else "N/A"

            # Ki·ªÉm tra th·ªùi gian b√†i vi·∫øt
            article_time = parse_znews_time(time_paper)
            if not is_in_time_range(article_time):
                print(f"B√†i vi·∫øt {article_href} kh√¥ng trong khung gi·ªù, b·ªè qua.")
                return True  # Return True ƒë·ªÉ kh√¥ng retry

            title_elem = soup.select_one('header.the-article-header > h1.the-article-title')
            title_paper = title_elem.get_text(strip=True) if title_elem else "Kh√¥ng c√≥ ti√™u ƒë·ªÅ"

            content_elems = soup.select('div.the-article-body p')
            content_paper = " ".join([p.get_text(strip=True) for p in content_elems if p])

            writer.writerow(["ZNews", article_href, category_name, 'Null', time_paper, title_paper, content_paper])
            crawled_urls.add(article_href)
            print(f"ƒê√£ crawl b√†i {article_href} - Th·ªùi gian: {time_paper}")
            return True
        except TimeoutException:
            print(f"Timeout khi t·∫£i {article_href}, th·ª≠ l·∫°i {attempt+1}/3")
            time.sleep(random.uniform(2, 5))
        except (NoSuchElementException, StaleElementReferenceException) as e:
            print(f"L·ªói ph·∫ßn t·ª≠ khi t·∫£i {article_href}: {e}")
            return False
        except Exception as e:
            print(f"L·ªói kh√¥ng x√°c ƒë·ªãnh khi t·∫£i {article_href}: {e}")
            return False
    return False

# üèÅ B·∫Øt ƒë·∫ßu qu√° tr√¨nh crawl
driver = init_driver()
crawled_urls = load_crawled_urls(csv_file)
article_count = 0
max_articles_before_restart = 100

# M·ªü file ·ªü ch·∫ø ƒë·ªô append ƒë·ªÉ kh√¥ng m·∫•t d·ªØ li·ªáu c≈©
file_mode = 'a' if crawled_urls else 'w'
write_header = not crawled_urls

try:
    driver.get(base_url)
    wait_for_element(driver, By.CSS_SELECTOR, 'div.page-wrapper', timeout=10)
    
    try:
        more_button = wait_for_element(driver, By.CSS_SELECTOR, 'li.more')
        if more_button:
            more_button.click()
            wait_for_element(driver, By.CSS_SELECTOR, 'ul.normal-category', timeout=5)
        else:
            print("Kh√¥ng t√¨m th·∫•y n√∫t 'More'")
    except Exception as e:
        print(f"L·ªói khi click n√∫t 'More': {e}")

    soup_panel = BeautifulSoup(driver.page_source, 'html.parser')
    soup_categories = soup_panel.select('div.page-wrapper > ul.normal-category > li > a')
    
    with open(csv_file, mode=file_mode, encoding='utf-8-sig', newline='') as file:
        writer = csv.writer(file)
        if write_header:
            writer.writerow(["Source", "URL", "Category", "Keyword", "Time", "Title", "Content"])
        
        for cate in soup_categories:
            category_url = f"{cate['href']}"
            category_name = cate.get_text(strip=True)
            
            print(category_name)
            if category_name in EXCLUDED_CATEGORIES:
                print(f"B·ªè qua danh m·ª•c: {category_name}")
                continue
            
            print(f"ƒêang x·ª≠ l√Ω danh m·ª•c: {category_name}")
            
            driver.get(category_url)
            wait_for_element(driver, By.CSS_SELECTOR, 'div.article-list', timeout=10)
            
            # ===========================================================
            article_hrefs = set()
            last_height = driver.execute_script("return document.body.scrollHeight")
            stop_scroll = False

            while not stop_scroll:
                soup_articles = BeautifulSoup(driver.page_source, 'html.parser')
                articles = soup_articles.select('div.article-list > article.article-item')
                print(f"T√¨m th·∫•y {len(articles)} b√†i trong trang {category_url}")

                for article in articles:
                    try:
                        article_href = article.select_one('p.article-thumbnail > a')['href']
                        time_elem = article.select_one('span.article-publish > span.date')
                        if time_elem:
                            time_text = time_elem.get_text(strip=True)
                            print('>>> time: ', time_text)
                            try:
                                article_date = datetime.strptime(time_text, "%d/%m/%Y").date()
                                print(f"B√†i {article_href}: Ng√†y {article_date}")
                                print(f'>>> article_date: {article_date} - current_date: {current_time.date()}')
                                
                                # Ch·ªâ l·∫•y b√†i trong ng√†y hi·ªán t·∫°i
                                if article_date == current_time.date():
                                    article_hrefs.add(article_href)
                                elif article_date < current_time.date():
                                    print(f"D·ª´ng scroll trong {category_name}, ph√°t hi·ªán b√†i c≈©: {article_date}")
                                    stop_scroll = True
                                    break
                                else:
                                    continue
                            except ValueError:
                                print(f"Kh√¥ng th·ªÉ parse ng√†y {time_text} cho b√†i {article_href}")
                                continue
                        else:
                            print(f"Kh√¥ng t√¨m th·∫•y th·∫ª ng√†y cho b√†i {article_href}")
                    except Exception as e:
                        print(f"L·ªói khi x·ª≠ l√Ω b√†i {article_href}: {e}")
                        continue

                if stop_scroll:
                    break

                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                wait_for_element(driver, By.CSS_SELECTOR, 'div.article-list', timeout=5)

                new_height = driver.execute_script("return document.body.scrollHeight")
                if new_height == last_height:
                    print(f"D·ª´ng scroll trong {category_name}, kh√¥ng c√≤n n·ªôi dung m·ªõi.")
                    break
                last_height = new_height
                
            print(f"T√¨m th·∫•y {len(article_hrefs)} b√†i ph√π h·ª£p trong {category_name}")
                
            for article_href in article_hrefs:
                if article_count >= max_articles_before_restart:
                    print("Kh·ªüi ƒë·ªông l·∫°i driver ƒë·ªÉ l√†m m·ªõi t√†i nguy√™n.")
                    driver.quit()
                    driver = init_driver()
                    article_count = 0
                
                if not crawl_article(driver, article_href, writer, crawled_urls):
                    print("Kh·ªüi ƒë·ªông l·∫°i driver do l·ªói nghi√™m tr·ªçng.")
                    driver.quit()
                    driver = init_driver()
                    continue
                
                article_count += 1
            # ===========================================================
except Exception as e:
    print(f"L·ªói ch√≠nh: {e}")
finally:
    driver.quit()

print("Ho√†n t·∫•t qu√° tr√¨nh thu th·∫≠p d·ªØ li·ªáu.")