from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from datetime import datetime, timedelta
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException
from bs4 import BeautifulSoup
import time
import csv
import random
import pytz
import re

base_url = 'https://vnexpress.net'
csv_file = 'dataset_paper_vnexpress.csv'

vn_timezone = pytz.timezone('Asia/Ho_Chi_Minh')
current_time = datetime.now(vn_timezone)

# X√°c ƒë·ªãnh khung gi·ªù c·ªë ƒë·ªãnh d·ª±a tr√™n gi·ªù hi·ªán t·∫°i
# Chia ng√†y th√†nh c√°c khung 3 ti·∫øng: 0-2, 3-5, 6-8, 9-11, 12-14, 15-17, 18-20, 21-23
current_hour = current_time.hour
time_slot_start_hour = (current_hour // 3) * 3  # L√†m tr√≤n xu·ªëng b·ªôi s·ªë c·ªßa 3

# T·∫°o khung gi·ªù: t·ª´ X:00:00 ƒë·∫øn X+2:59:59
time_start = current_time.replace(hour=time_slot_start_hour, minute=0, second=0, microsecond=0)
time_end = time_start.replace(hour=time_slot_start_hour + 2, minute=59, second=59, microsecond=999999)

print(f"Khung gi·ªù crawl: {time_start.strftime('%Y-%m-%d %H:%M:%S')} ƒë·∫øn {time_end.strftime('%Y-%m-%d %H:%M:%S')}")

# üõ† H√†m kh·ªüi t·∫°o driver
def init_driver():
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--disable-extensions")
    options.add_argument("--ignore-certificate-errors")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--dns-prefetch-disable")
    driver = webdriver.Chrome(options=options)
    driver.set_page_load_timeout(180)
    return driver

# üõ† H√†m ch·ªù ph·∫ßn t·ª≠
def wait_for_element(driver, by, value, timeout=10):
    try:
        return WebDriverWait(driver, timeout).until(EC.presence_of_element_located((by, value)))
    except TimeoutException:
        return None

# üõ† H√†m ƒë·ªçc c√°c URL ƒë√£ crawl t·ª´ file CSV
def load_crawled_urls(csv_file):
    crawled_urls = set()
    try:
        with open(csv_file, mode='r', encoding='utf-8-sig') as file:
            reader = csv.reader(file)
            next(reader, None)
            for row in reader:
                if len(row) >= 2:
                    crawled_urls.add(row[1])
    except FileNotFoundError:
        pass
    return crawled_urls

# üõ† H√†m parse th·ªùi gian t·ª´ text VNExpress
def parse_vnexpress_time(time_text):
    """
    Parse th·ªùi gian t·ª´ VNExpress format:
    - "Th·ª© b·∫£y, 23/11/2024, 02:30 (GMT+7)"
    - "H√¥m qua, 02:30"
    - "2 gi·ªù tr∆∞·ªõc"
    - "30 ph√∫t tr∆∞·ªõc"
    """
    try:
        time_text = time_text.strip()
        
        # Format ƒë·∫ßy ƒë·ªß: "Th·ª© b·∫£y, 23/11/2024, 02:30 (GMT+7)"
        match = re.search(r'(\d{1,2})/(\d{1,2})/(\d{4}),?\s*(\d{1,2}):(\d{2})', time_text)
        if match:
            day, month, year, hour, minute = match.groups()
            return datetime(int(year), int(month), int(day), int(hour), int(minute), tzinfo=vn_timezone)
        
        # "H√¥m qua, HH:MM"
        if "H√¥m qua" in time_text or "h√¥m qua" in time_text:
            match = re.search(r'(\d{1,2}):(\d{2})', time_text)
            if match:
                hour, minute = match.groups()
                yesterday = current_time - timedelta(days=1)
                return yesterday.replace(hour=int(hour), minute=int(minute), second=0, microsecond=0)
        
        # "X gi·ªù tr∆∞·ªõc"
        match = re.search(r'(\d+)\s*gi·ªù tr∆∞·ªõc', time_text)
        if match:
            hours_ago = int(match.group(1))
            return current_time - timedelta(hours=hours_ago)
        
        # "X ph√∫t tr∆∞·ªõc"
        match = re.search(r'(\d+)\s*ph√∫t tr∆∞·ªõc', time_text)
        if match:
            minutes_ago = int(match.group(1))
            return current_time - timedelta(minutes=minutes_ago)
        
        print(f"Kh√¥ng parse ƒë∆∞·ª£c th·ªùi gian: {time_text}")
        return None
        
    except Exception as e:
        print(f"L·ªói khi parse th·ªùi gian '{time_text}': {e}")
        return None

# üõ† H√†m ki·ªÉm tra b√†i vi·∫øt c√≥ trong khung gi·ªù kh√¥ng
def is_in_time_range(article_time):
    """Ki·ªÉm tra xem th·ªùi gian b√†i vi·∫øt c√≥ n·∫±m trong khung gi·ªù [time_start, time_end] kh√¥ng"""
    if not article_time:
        return False
    return time_start <= article_time <= time_end

# üõ† H√†m crawl b√†i b√°o
def crawl_article(driver, article_url, category_name, writer, crawled_urls):
    if article_url in crawled_urls:
        print(f"B√†i {article_url} ƒë√£ ƒë∆∞·ª£c crawl, b·ªè qua.")
        return True

    for attempt in range(3):
        try:
            driver.get(article_url)
            time.sleep(2)
            
            soup_detail_article = BeautifulSoup(driver.page_source, 'html.parser')
            
            keyword_elems = driver.find_elements(By.CLASS_NAME, 'item-tag')
            
            time_article = soup_detail_article.select_one('div.sidebar-1 > div.header-content > span.date, span.date')
            title_article = soup_detail_article.select_one('div.sidebar-1 > h1.title-detail, h1.title-detail')
            para_head_article = soup_detail_article.select_one('div.sidebar-1 > p.description, p.description')
            para_main_article = soup_detail_article.select('div.sidebar-1 > article.fck_detail > p.Normal, article.fck_detail > p.Normal, p.Normal')

            time_text = time_article.get_text(strip=True) if time_article else 'N/A'
            title_text = title_article.get_text(strip=True) if title_article else 'N/A'
            para_head_text = para_head_article.get_text(strip=True) if para_head_article else ''
            para_main_text = " ".join([p.get_text(strip=True) for p in para_main_article]) if para_main_article else ''
            keyword_paper = ",".join([a.text for a in keyword_elems])

            full_content = f"{para_head_text} {para_main_text}".strip()

            if not full_content:
                print(f"Kh√¥ng t√¨m th·∫•y n·ªôi dung cho b√†i vi·∫øt: {article_url}")
                return False

            # Ki·ªÉm tra th·ªùi gian b√†i vi·∫øt
            article_time = parse_vnexpress_time(time_text)
            if not is_in_time_range(article_time):
                print(f"B√†i vi·∫øt {article_url} kh√¥ng trong khung gi·ªù, b·ªè qua.")
                return True  # Return True ƒë·ªÉ kh√¥ng retry

            writer.writerow(["VN Express", article_url, category_name, keyword_paper, time_text, title_text, full_content])
            crawled_urls.add(article_url)
            print(f"ƒê√£ crawl b√†i {article_url} - Th·ªùi gian: {time_text}")
            return True
            
        except TimeoutException:
            print(f"Timeout khi t·∫£i {article_url}, th·ª≠ l·∫°i {attempt+1}/3")
            time.sleep(random.uniform(2, 5))
        except (NoSuchElementException, StaleElementReferenceException) as e:
            print(f"L·ªói ph·∫ßn t·ª≠ khi t·∫£i {article_url}: {e}")
            return False
        except Exception as e:
            print(f"L·ªói kh√¥ng x√°c ƒë·ªãnh khi t·∫£i {article_url}: {e}")
            return False
    return False

# üèÅ B·∫Øt ƒë·∫ßu qu√° tr√¨nh crawl
driver = init_driver()
crawled_urls = load_crawled_urls(csv_file)
article_count = 0
max_articles_before_restart = 100

try:
    driver.get(base_url)
    wait_for_element(driver, By.CSS_SELECTOR, 'ul.parent > li', timeout=10)
    
    soup_categories_paper = BeautifulSoup(driver.page_source, 'html.parser')
    soup_categories = soup_categories_paper.select('ul.parent > li')

    # M·ªü file ·ªü ch·∫ø ƒë·ªô append ƒë·ªÉ kh√¥ng m·∫•t d·ªØ li·ªáu c≈©
    file_mode = 'a' if crawled_urls else 'w'
    write_header = not crawled_urls
    
    with open(csv_file, mode=file_mode, encoding='utf-8-sig', newline='') as file:
        writer = csv.writer(file)
        if write_header:
            writer.writerow(["Source", "URL", "Category", "Keyword", "Time", "Title", "Content"])
        
        if soup_categories:
            for li in soup_categories:
                ul_tags = li.select('ul.sub')
                for ul_tag in ul_tags:
                    sub_lis = ul_tag.find_all('li')
                    for sub_li in sub_lis:
                        a_tag = sub_li.select_one('a')
                        if not a_tag:
                            continue
                            
                        href_a_sub_li = a_tag.get("href", "")
                        if not href_a_sub_li:
                            continue
                            
                        name_category = a_tag.get_text(strip=True)
                        if not href_a_sub_li.startswith('http'):
                            href_a_sub_li = base_url + href_a_sub_li

                        try:
                            print(f"ƒêang truy c·∫≠p danh m·ª•c: {name_category} ({href_a_sub_li})")
                            driver.get(href_a_sub_li)
                            wait_for_element(driver, By.CSS_SELECTOR, 'div.list-news-subfolder > article.item-news, article.item-news', timeout=10)
                            
                            soup_paper = BeautifulSoup(driver.page_source, 'html.parser')

                            # T√¨m ph√¢n trang
                            pagination_links = soup_paper.select('div.button-page a')
                            page_numbers = [int(link.text) for link in pagination_links if link.text.isdigit()]
                            last_page = max(page_numbers) if page_numbers else 1

                            print(f"T√¨m th·∫•y {last_page} trang cho danh m·ª•c: {name_category}")

                            for page in range(1, last_page + 1):
                                stop_category = False
                                try:
                                    page_url = f'{href_a_sub_li}-p{page}' if page > 1 else href_a_sub_li
                                    print(f"ƒêang x·ª≠ l√Ω trang {page}/{last_page}: {page_url}")
                                    
                                    driver.get(page_url)
                                    wait_for_element(driver, By.CSS_SELECTOR, 'div.list-news-subfolder > article.item-news, article.item-news', timeout=10)
                                    
                                    soup_data_paper = BeautifulSoup(driver.page_source, 'html.parser')
                                    data_paper = soup_data_paper.select('div.list-news-subfolder > article.item-news, article.item-news')

                                    if not data_paper:
                                        print(f"Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt n√†o trong trang {page}")
                                        continue

                                    # Thu th·∫≠p danh s√°ch URL t·ª´ trang hi·ªán t·∫°i
                                    article_urls = []
                                    for data in data_paper:
                                        href_article = data.select_one('h2.title-news > a, h3.title-news > a, a.title-news')
                                        if href_article:
                                            href_article_data = href_article.get("href", "")
                                            if not href_article_data:
                                                continue
                                                
                                            if not href_article_data.startswith('http'):
                                                href_article_data = base_url + href_article_data

                                            if href_article_data not in crawled_urls:
                                                article_urls.append(href_article_data)

                                    # Crawl c√°c b√†i vi·∫øt ƒë√£ thu th·∫≠p
                                    for article_url in article_urls:
                                        # Ki·ªÉm tra v√† kh·ªüi ƒë·ªông l·∫°i driver n·∫øu c·∫ßn
                                        if article_count >= max_articles_before_restart:
                                            print("Kh·ªüi ƒë·ªông l·∫°i driver ƒë·ªÉ l√†m m·ªõi t√†i nguy√™n.")
                                            driver.quit()
                                            driver = init_driver()
                                            article_count = 0
                                        
                                        if not crawl_article(driver, article_url, name_category, writer, crawled_urls):
                                            print("Kh·ªüi ƒë·ªông l·∫°i driver do l·ªói nghi√™m tr·ªçng.")
                                            driver.quit()
                                            driver = init_driver()
                                            continue
                                        
                                        article_count += 1
                                        
                                        # Ngh·ªâ ng·∫´u nhi√™n ƒë·ªÉ tr√°nh b·ªã ch·∫∑n
                                        sleep_time = random.uniform(1, 3)
                                        time.sleep(sleep_time)

                                    if stop_category:
                                        print(f"D·ª´ng t·∫°i trang {page} c·ªßa danh m·ª•c {name_category}")
                                        break
                                    
                                    # Ngh·ªâ gi·ªØa c√°c trang
                                    sleep_time = random.uniform(2, 4)
                                    print(f"Ngh·ªâ {sleep_time:.2f} gi√¢y tr∆∞·ªõc khi ti·∫øp t·ª•c...")
                                    time.sleep(sleep_time)

                                except Exception as e:
                                    print(f"L·ªói khi t·∫£i trang {page_url}: {e}")
                                    continue

                        except Exception as e:
                            print(f"L·ªói khi l·∫•y danh m·ª•c {href_a_sub_li}: {e}")
                            continue

        else:
            print('Kh√¥ng t√¨m th·∫•y menu')

except Exception as e:
    print(f"L·ªói ch√≠nh: {e}")

finally:
    driver.quit()

print("Ho√†n t·∫•t qu√° tr√¨nh thu th·∫≠p d·ªØ li·ªáu.")